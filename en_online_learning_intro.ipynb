{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-extensive introduction to Online Machine Learning\n",
    "\n",
    "\n",
    "**Saulo Martiello Mastelini** (saulomastelini@gmail.com)\n",
    "\n",
    "Contact:\n",
    "\n",
    "- [Website](https://smastelini.github.io)\n",
    "- [Github](https://github.com/smastelini)\n",
    "- [Linkedin](https://www.linkedin.com/in/smastelini/)\n",
    "- [ResearchGate](https://www.researchgate.net/profile/Saulo-Mastelini)\n",
    "\n",
    "Copyright (c) 2022\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**\n",
    "\n",
    "As the title implies, this material is not an extensive introduction to the topic. It is just my humble attempt to present a general overview of decades worth of research in an ever-expanding area.\n",
    "\n",
    "Every process takes time. Therefore, a few minutes or hours are not enough to explore a whole research area. The idea is to find the balance between diving too deep into a topic and being too superficial. This hands-on talk is not formal, so feel free to interrupt me and ask questions anytime.\n",
    "\n",
    "---\n",
    "\n",
    "**If you want to explore further**\n",
    "\n",
    "If you want to learn more about the topics discussed in this notebook, I suggest:\n",
    "\n",
    "- [MOA book](https://moa.cms.waikato.ac.nz/book-html/): an open-access book that discusses a lot of themes related to data streams\n",
    "- [River documentation](https://riverml.xyz/): it has plenty of examples, tutorials, and theoretical resources. It is constantly updated and expanded.\n",
    "\n",
    "If you have a specific question that is not covered in the documentation, you can always open a new _Discussion_ on Github. For sure somebody will help you! To do that, you need to head to the [River](https://github.com/online-ml/river) repository and find the discussion tab.\n",
    "\n",
    "Contributions are always welcome. River is open source and kept by a community. Even though you might not have a technical background, it is always possible to help. Fixing and expanding the documentation is just an example of possible ways to get involved. If you find a bug, please let us know! üòÅ\n",
    "\n",
    "---\n",
    "\n",
    "**About River**\n",
    "\n",
    "River is an open-source project focused on online machine learning and stream mining. It is the result of a merger between two preceding open source projects:\n",
    "\n",
    "- creme\n",
    "- scikit-multiflow\n",
    "\n",
    "creme and scikit-multiflow had a lot of overlap and also different strengths and weaknesses. After a long time of planning and discussing core design aspects, the maintainers of both projects joined forces and created River.\n",
    "\n",
    "Hence, River has the best of both worlds and it is the result of years of learned lessons in the preceding tools. River is focused on both researchers and practicioneers. A lot of people help River keep growing, but the core development team is spread between France, New Zealand, Vietnam, and Brazil.\n",
    "\n",
    "---\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Online learning? Why?\n",
    "2. Batch vs. Online\n",
    "3. Building blocks: some examples\n",
    "4. Why dictionaries?\n",
    "5. How to evaluate an online machine learning model?\n",
    "    - `progressive_val_score`\n",
    "    - label delay\n",
    "6. Concept drift\n",
    "7. Examples of algorithms\n",
    "    1. Classification\n",
    "        1. Hoeffding Tree\n",
    "        2. Adaptive Random Forest\n",
    "    2. Regression\n",
    "        1. **Hoeffding Tree**\n",
    "        2. **AMRules**\n",
    "    3. Clustering\n",
    "        1. k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Latest released version\n",
    "# !pip install river\n",
    "\n",
    "# Development version\n",
    "#!pip install git+https://github.com/online-ml/river --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Online Learning? Why?\n",
    "\n",
    "Q: Why should somebody care about updating models online? What about just training them once and using them?\n",
    "A: Well, that is indeed enough for most cases.\n",
    "\n",
    "Nonetheless, imagine that:\n",
    "\n",
    "- The amount of data instances is huge\n",
    "- It is not possible to store everything\n",
    "- The available computational power is limited\n",
    "    - CPUs\n",
    "    - Memory\n",
    "    - Battery\n",
    "- Data is non-stationary and/or evolves through time\n",
    "\n",
    "Q: Is it possible to use traditional machine learning in these cases?\n",
    "A: Yes!\n",
    "\n",
    "One can still use traditional or batch machine learning if:\n",
    "\n",
    "- Data is stationary, i.e., a sufficiently large sample is enough to achieve generalization\n",
    "\n",
    "or\n",
    "\n",
    "- The speed at which data is produced or collected is not too high\n",
    "    - In these cases, the batch-incremental approach is a possible solution\n",
    "\n",
    "## 1.1 Batch-incremental\n",
    "\n",
    "A batch machine learning model is retrained in this strategy at regular intervals. Hence, we must define a training window by following one among the possible approaches:\n",
    "\n",
    "<img src=\"time_windows.png\">\n",
    "\n",
    "**Fonte:** Adapted from:\n",
    "\n",
    "> Carnein, M. and Trautmann, H., 2019. Optimizing data stream representation: An extensive survey on stream clustering algorithms. Business & Information Systems Engineering, 61(3), pp.277-297.\n",
    "\n",
    "- *Landmarks* are the most common choice for batch-incremental applications. The window length is the central concern.\n",
    "    - The current model may become outdated if the window is too large\n",
    "    - The model may fail to capture the underlying patterns in the data if the window is too small.\n",
    "    - Concept drift is a serious problem\n",
    "        - Drifts do not typically occur at predefined and regular intervals\n",
    "    \n",
    "**Attention**: batch-incremental != mini-batch.\n",
    "\n",
    "Artificial neural networks can be trained incrementally or progressively, usually relying on mini-batches of data.\n",
    "\n",
    "Challenges such as \"catastrophic forgetting\" are one of the main concerns tackled in the **continual learning** research field.\n",
    "\n",
    "## 1.2. It is worth noting\n",
    "\n",
    "Data streams are not necessarily, time series.\n",
    "\n",
    "Q: What is the difference between data streams and time series?\n",
    "A: Data streams do not necessarily have explicit temporal dependencies like time series. For instance, sensor networks.\n",
    "    - Varying transmission speeds\n",
    "    - Sensor failure\n",
    "    - Network expansion\n",
    "    - And so on...\n",
    "    Hence, the arrival order does not matter... much, but it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch vs. Online\n",
    "\n",
    "The River website has a nice [tutorial](https://riverml.xyz/latest/examples/batch-to-online/) on going from batch to online ML. But let's give a general overview of the differences.\n",
    "\n",
    "A typical batch ML evaluation pipeline might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = load_wine()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "kf = KFold(shuffle=True, random_state=8, n_splits=10)\n",
    "\n",
    "accs = []\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_tr, X_ts = X[train], X[test]\n",
    "    y_tr, y_ts = y[train], y[test]\n",
    "    \n",
    "    dt  = DecisionTreeClassifier(max_depth=5, random_state=93)\n",
    "    dt.fit(X_tr, y_tr)\n",
    "    \n",
    "    accs.append(accuracy_score(y_ts, dt.predict(X_ts)))\n",
    "\n",
    "print(f\"Mean accuracy: {sum(accs) / len(accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded in the memory and entirely available for inspection. The decision tree algorithm is allowed to perform multiple passes over the (training) data. Validation data is never used for training.\n",
    "\n",
    "In the end, we might take the complete dataset (training + validation) to build a \"final model\", given that we have already found a good set of hyperparameters. Once trained, this model will be used to predict the types of wine samples.\n",
    "\n",
    "Let's see what an online ML evaluation might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "from river import stream\n",
    "from river import tree\n",
    "\n",
    "\n",
    "acc = metrics.Accuracy()\n",
    "ht = tree.HoeffdingTreeClassifier(max_depth=5, grace_period=20)\n",
    "\n",
    "for x, y in stream.iter_sklearn_dataset(load_wine()):\n",
    "    # The evaluation metric is evaluated before the model actually learns from the instance\n",
    "    acc.update(y, ht.predict_one(x))\n",
    "    # The model is updated one instance at a time\n",
    "    ht.learn_one(x, y)\n",
    "\n",
    "print(f\"Accuracy: {acc.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the input data sequentially. Data might be loaded on demand from the disk, a web server, or anywhere.\n",
    "Data does not need to fit into the available memory.\n",
    "\n",
    "Each instance is first used for testing and then to update the learning model. Everything works in an instance-by-instance regimen.\n",
    "\n",
    "If the underlying process is guaranteed to be stationary, we could shuffle the data before passing it to the model.\n",
    "\n",
    "**Note:** we cannot directly compare both the obtained accuracy values, as the evaluation strategies are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building blocks: some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from river import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After first glancing at the differences, let's take things slowly and reflect on the building blocks necessary to perform Online Machine Learning.\n",
    "\n",
    "Let's suppose we want to keep statistics for continually arriving data. For instance, we want to calculate the mean and variance.\n",
    "\n",
    "\n",
    "Time to simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rng = random.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "values = []\n",
    "stds_batch = []\n",
    "\n",
    "for _ in range(50000):\n",
    "    v = rng.gauss(5, 3)\n",
    "    values.append(v)\n",
    "\n",
    "    stds_batch.append(np.std(values, ddof=1) if len(values) > 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stds_incr = []\n",
    "var = stats.Var(ddof=1)\n",
    "\n",
    "for _ in range(50000):\n",
    "    v = rng.gauss(5, 3)\n",
    "    var.update(v)\n",
    "    stds_incr.append(var.get() ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot faster! But does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_errors = 0\n",
    "\n",
    "for batch, incr in zip(stds_batch, stds_incr):\n",
    "    s_errors += (batch - incr)\n",
    "\n",
    "s_errors, s_errors / len(stds_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this is convincing! River's [stats](https://riverml.xyz/dev/api/overview/#stats) module has a lot of tools to calculate statistics üßê\n",
    "\n",
    "Many of these things are the building blocks of Online Machine Learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "**Practical example: Variance using the Welford algorithm**\n",
    "\n",
    "- We need some variables:\n",
    "    - $n$: number of observations\n",
    "    - $\\overline{x}_n$: the sample mean, after $n$ observations\n",
    "    - $M_{2, n}$: second-order statistic\n",
    "- The variables are initialized as follows:\n",
    "    - $\\overline{x}_{0} \\leftarrow 0$\n",
    "    - $M_{2,0} \\leftarrow 0$\n",
    "- The variables are updated using the following expressions:\n",
    "    - $\\overline{x}_n = \\overline{x}_{n-1} + \\dfrac{x_n - \\overline{x}_{n-1}}{n}$\n",
    "    - $M_{2,n} = M_{2,n-1} + (x_n - \\overline{x}_{n-1})(x_n - \\overline{x}_n)$\n",
    "- The sample variance is obtained using: $s_n^2 = \\dfrac{M_{2,n}}{n-1}$, for every $n > 1$\n",
    "- We also get a robust mean estimator for free! ü§ì\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why dictionaries (or why using a sparse data representation)?\n",
    "\n",
    "In River, we use dictionaries as the primary data type.\n",
    "\n",
    "Dictionaries:\n",
    "\n",
    "- Key x value: keys are unique\n",
    "- Values accessed via keys instead of indices\n",
    "- Sparse\n",
    "- There is no explicit ordering\n",
    "- Dynamic!\n",
    "- Mixed data types\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "x = {\n",
    "    \"potato\": 3,\n",
    "    \"car\": 2,\n",
    "    \"data\": datetime.now(),\n",
    "    \"yes_or_no\": \"yes\"\n",
    "}\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"one extra\"] = True\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x[\"data\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: dictionaries are very similar to JSON.\n",
    "\n",
    "Let's compare dictionaries with the traditional approach, based on arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X[0, :], data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0], data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to put sklearn to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = X[:-2, :], y[:-2]\n",
    "X_ts, y_ts = X[-2:, :], y[-2:]\n",
    "\n",
    "X_tr.shape, X_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_tr, y_tr)\n",
    "\n",
    "nb.predict(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if one feature was missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nb.predict(X_ts[:, 1:])\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That type of situation is not uncommon in online scenarios. New sensors appear, some fail, and so on. So we must be able to deal with this kind of situation.\n",
    "\n",
    "The majority of the models in River can deal with missing and emerging features! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import naive_bayes\n",
    "\n",
    "gnb = naive_bayes.GaussianNB()\n",
    "dataset = stream.iter_sklearn_dataset(load_wine())\n",
    "\n",
    "rng = random.Random(42)\n",
    "\n",
    "# Probability of ignoring a feature\n",
    "del_chance = 0.2\n",
    "\n",
    "n_incomplete = 0\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    if i == 176:\n",
    "        break\n",
    "    \n",
    "    x_copy = x.copy()\n",
    "    aux = 0\n",
    "    for xi in x:\n",
    "        if rng.random() <= del_chance:\n",
    "            del x_copy[xi]\n",
    "            aux = 1\n",
    "        \n",
    "        # Update the counter of incomplete instances\n",
    "        n_incomplete += aux\n",
    "    \n",
    "    gnb.learn_one(x_copy, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba_one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explicitly modify this last example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(dataset)\n",
    "list(x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make a copy and delete some features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.copy()\n",
    "\n",
    "del x_copy[\"malic_acid\"]\n",
    "del x_copy[\"hue\"]\n",
    "del x_copy[\"flavanoids\"]\n",
    "\n",
    "x_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will our model work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba_one(x_copy), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if new features appeared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"1st extra\"] = 7.89\n",
    "x[\"2nd extra\"] = 2\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.learn_one(x, y)\n",
    "\n",
    "gnb.predict_one({\"1st extra\": 7.8, \"2nd extra\": 1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model implements different strategies to deal with missing or emerging features.\n",
    "\n",
    "In our example, \"1\" was the majority class, and so was the prediction of GaussianNB. That is the best it can do since there is not enough information about the new features. But these new features are already part of the model and will be updated with more observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How to evaluate models?\n",
    "\n",
    "\n",
    "In every example presented so far, when a new instance arrives, we first make a prediction and then use the new datum to update the model.\n",
    "No cross-validation, leave-one-out, and so on.\n",
    "\n",
    "This evaluation strategy is close to a real-world scenario: usually, we first get the inputs without labels, and predictions must be made. After some time, class labels arrive.\n",
    "In our examples, the label is \"revealed\" after the model makes a prediction. A delay exists between predicting and getting the label in an even more realistic evaluation scenario. Sometimes, the label never arrives for some instances.\n",
    "\n",
    "We call this type of evaluation strategy _progressive validation_ or _prequential_ evaluation.\n",
    "\n",
    "I suggest checking this [blog post from Max Halford](https://maxhalford.github.io/blog/online-learning-evaluation/), for more details on that matter.\n",
    "\n",
    "In River, we have a utility function `progressive_val_score` in the `evaluate` module that handles all the situations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river.datasets import synth\n",
    "\n",
    "\n",
    "def label_delay(x, y):\n",
    "    return rng.randint(0, 100)\n",
    "\n",
    "\n",
    "rng = random.Random(8)\n",
    "dataset = synth.RandomRBF(seed_sample=7, seed_model=9)\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "\n",
    "# We can combine metrics using pipeline operators\n",
    "metric = metrics.Accuracy() + metrics.MicroF1() + metrics.BalancedAccuracy()\n",
    "\n",
    "evaluate.progressive_val_score(\n",
    "    dataset=dataset.take(50000),\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    print_every=5000,\n",
    "    show_memory=True,\n",
    "    show_time=True,\n",
    "    delay=label_delay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Concept drift\n",
    "\n",
    "One of the main concerns in online machine learning is the fact that data distribution may not be stationary. What does that mean?\n",
    "\n",
    "Let's first think about an example of stationary distribution:\n",
    "\n",
    "> Big tech company X released a new neural network for the Y problem with 3 zillion parameters, trained for 6 months using enough energy to power up multiple cities. The training dataset had Z terabytes...\n",
    "\n",
    "Well, the data does not change. Linguistic rules (in NLP) or visual semantics don't usually vary or evolve. Everything is static under the same data collection policy.\n",
    "\n",
    "A dog will always be a dog. A word has a limited set of synonyms, and so on. The rule of the game does not change. But even in these scenarios, there are exceptions. What if the rules changed?\n",
    "\n",
    "These changes or concept drift may occur in real-world problems. For example:\n",
    "\n",
    "Consumer buying pattern (toilet paper, masks, and hand sanitizer at the beginning of Covid pandemics);\n",
    "Renewable energy production: sunlight and wind are not predictable;\n",
    "traffic and routes\n",
    "\n",
    "An entire research field in online machine learning is devoted to creating concept drift detectors and learning algorithms capable of adapting to changes in the data distribution.\n",
    "\n",
    "I am not an expert on this topic, but I will try to give you a simple example of how to apply a drift detector.\n",
    "\n",
    "Let's suppose we have a classification problem and are monitoring our model's predictive performance. We denote by $0$ the cases where the model correctly classifies an instance and by $1$ the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng = random.Random(8)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(rng.choices([0, 1], weights=[0.7, 0.3])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed these values to a drift detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import drift\n",
    "\n",
    "detector = drift.ADWIN(delta=0.01)\n",
    "\n",
    "vals = rng.choices([0, 1], weights=[0.7, 0.3], k=500)\n",
    "for i, v in enumerate(vals):\n",
    "    detector.update(v)\n",
    "    \n",
    "    if detector.drift_detected:\n",
    "        print(f\"Drift detected: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the data distribution changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = drift.ADWIN(delta=0.05)\n",
    "\n",
    "vals = rng.choices([0, 1], weights=[0.7, 0.3], k=500)\n",
    "vals.extend(rng.choices([0, 1], weights=[0.2, 0.8], k=500))\n",
    "for i, v in enumerate(vals):\n",
    "    detector.update(v)\n",
    "    \n",
    "    if detector.drift_detected:\n",
    "        print(f\"Drift detected: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADWIN is one of the most utilized drift detectors, but there many other algorithms. Non-supervised, semi-supervised, multivariate, and so on.\n",
    "Usually, detectors are used as components of predictive models. Each models applies drift detectors in a different manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Algorithm examples\n",
    "\n",
    "I will present some examples of classification, regression, and clustering algorithms for reference. The API access is always the same, so you can try your luck and check other examples in the documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Classification\n",
    "\n",
    "Algorithms projected for binary classification can be extended to the multiclass case by relying on the tools available in the `multiclass` module:\n",
    "\n",
    "- `OneVsOneClassifier`\n",
    "- `OneVsRestClassifier`\n",
    "- `OutputCodeClassifier`\n",
    "\n",
    "River also have basics tools to handle multi-output tasks. Any contributions are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Hoeffding Trees\n",
    "\n",
    "One of the most popular families of online machine learning algorithms. They take this name because the statistical measure called Hoeffding bound is used to define when splits are performed. This heuristic ensures the decisions taken incrementally are similar to those performed by a batch decision tree algorithm.\n",
    "\n",
    "\n",
    "There are three main variants of Hoeffding Trees:\n",
    "\n",
    "- Hoeffding Tree: vanilla version\n",
    "- Hoeffding Adaptive Tree: adds drift detectors to each decision node. If a drift is detected, a new subtree is trained in the background and eventually may replace the affected tree branch.\n",
    "- Extremely Fast Decision Tree: quickly deploys splits but later revisits and improves its own decisions.\n",
    "\n",
    "**Main hyperparameters:**\n",
    "\n",
    "- `grace_period`: the interval between split attempts.\n",
    "- `delta`: the split significance parameter. The split confidence `1 - delta`.\n",
    "- `max_depth`: max height a tree might have.\n",
    "\n",
    "I wrote a [tutorial](https://riverml.xyz/dev/user-guide/on-hoeffding-trees/) on Hoeffding Trees, where you can find more details about the algorithms.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from river import tree\n",
    "\n",
    "\n",
    "dataset = synth.RandomRBFDrift(\n",
    "    seed_model=7, seed_sample=8, change_speed=0.0001, n_classes=3,\n",
    ").take(15000)\n",
    "model = tree.HoeffdingAdaptiveTreeClassifier(seed=42)\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric, print_every=1000, show_memory=True, show_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the tree structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how decisions are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = synth.RandomRBFDrift(\n",
    "    seed_model=7, seed_sample=8, change_speed=0.0001, n_classes=3,\n",
    ").take(15000)\n",
    "\n",
    "x, y = next(dataset)\n",
    "\n",
    "print(model.debug_one(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Adaptive Random Forest\n",
    "\n",
    "Adaptive random forest (ARF) is an incremental version of Random Forests that combines the following ingredients:\n",
    "\n",
    "- Randomized Hoeffding Trees as base learners\n",
    "- Drifts detectors for each tree\n",
    "    - New trees are trained in the background when drifts are detected\n",
    "- Online bagging\n",
    "\n",
    "ARFs have all the parameters of HTs and also some extra critical parameters:\n",
    "\n",
    "- `warning_detector` and `drift_detector`\n",
    "- `n_models`: the number of trees\n",
    "- `max_features`: the maximum number of features considered during split attempts at each decision node\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import ensemble\n",
    "\n",
    "\n",
    "dataset = synth.RandomRBFDrift(\n",
    "    seed_model=7, seed_sample=8, change_speed=0.0001, n_classes=3,\n",
    ").take(15000)\n",
    "\n",
    "model = ensemble.AdaptiveRandomForestClassifier(seed=8)\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric, print_every=1000, show_memory=True, show_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Regression\n",
    "\n",
    "\n",
    "I will use the same dataset for every regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_friedman():\n",
    "    return synth.Friedman(seed=101).take(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(get_friedman())\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Hoeffding Tree\n",
    "\n",
    "(I research this topic)\n",
    "\n",
    "We have three main types of HTs for regression tasks:\n",
    "\n",
    "- `HoeffdingTreeRegressor`: vanilla regressor.\n",
    "- `HoeffdingAdaptiveTreeRegressor`: the regression counterpart of the adaptive classification tree.\n",
    "- `iSOUPTreeRegressor`: Hoeffding Tree for multi-target regression tasks\n",
    "\n",
    "Besides the parameters presented in the classification version, other important parameters are:\n",
    "\n",
    "- `leaf_prediction`: the prediction strategy (regression or model tree)\n",
    "- `leaf_model`: the regression model used in model trees' leaves\n",
    "- `splitter`: the decision split algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from river import preprocessing\n",
    "\n",
    "# We can combine multiple metrics in our report\n",
    "metric = metrics.MAE() + metrics.RMSE() + metrics.R2()\n",
    "model = preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor()\n",
    "\n",
    "evaluate.progressive_val_score(\n",
    "    dataset=get_friedman(),\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    show_memory=True,\n",
    "    show_time=True,\n",
    "    print_every=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can inspect how decisions are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(get_friedman())\n",
    "\n",
    "print(model.debug_one(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. AMRules\n",
    "\n",
    "Adaptive Model Rules.\n",
    "\n",
    "(I also research this topic)\n",
    "\n",
    "Creates decision rules by relying on the Hoeffding Bound. AMRules also has anomaly detection capabilities to \"skip\" anomalous training samples.\n",
    "\n",
    "It has parameters similar to those of HTs:\n",
    "\n",
    "- `n_min`: equivalent to `grace_period`\n",
    "- `pred_type`: equivalent to `leaf_prediction`\n",
    "- `pred_model`: equivalent to `leaf_model`\n",
    "- `splitter`\n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `m_min`: minimum number of instances to observe before detecting anomalies.\n",
    "- `drift_detector`: the drift detection algorithm used by each rule.\n",
    "- `anomaly_threshold`: threshold to decide whether or not an instance is anomalous (the smaller the score value, the more anomalous the instance is).\n",
    "- `ordered_rule_set`: defines whether only the first rule is used for detection (when set to `True`) or all the rules are used (`False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from river import rules\n",
    "\n",
    "metric = metrics.MAE() + metrics.RMSE() + metrics.R2()\n",
    "model = preprocessing.StandardScaler() | rules.AMRules(\n",
    "    splitter=tree.splitter.TEBSTSplitter(digits=1),  #  <- this is part of my research\n",
    "    drift_detector=drift.ADWIN(),\n",
    "    ordered_rule_set=False,\n",
    "    m_min=100,\n",
    "    delta=0.01\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(\n",
    "    dataset=get_friedman(),\n",
    "    model=model,\n",
    "    metric=metric,\n",
    "    show_memory=True,\n",
    "    show_time=True,\n",
    "    print_every=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(get_friedman())\n",
    "\n",
    "print(model.debug_one(x))\n",
    "print(f\"True label: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = model[\"StandardScaler\"].transform_one(x)\n",
    "\n",
    "model[\"AMRules\"].anomaly_score(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Clustering\n",
    "\n",
    "Incremental algorithms must adapt to changes in the data. For instance, new clusters might appear, some might disappear. I will show one example of algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. k-Means\n",
    "\n",
    "There are multiple incremental versions of k-Means out there. The version available in River adds a parameter called `halflife` which controls the the intensity of the incremental updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import cluster\n",
    "\n",
    "metric = metrics.Silhouette()\n",
    "model = cluster.KMeans(seed=7)\n",
    "\n",
    "\n",
    "for x, _ in get_friedman():\n",
    "    metric.update(x, model.predict_one(x), model.centers)\n",
    "    model.learn_one(x)\n",
    "\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = metrics.Silhouette()\n",
    "model = cluster.KMeans(n_clusters=3, seed=7)\n",
    "\n",
    "\n",
    "for x, _ in get_friedman():\n",
    "    metric.update(x, model.predict_one(x), model.centers)\n",
    "    model.learn_one(x)\n",
    "\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And increase the `halflife` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metrics.Silhouette()\n",
    "model = cluster.KMeans(n_clusters=3, seed=7, halflife=0.7)\n",
    "\n",
    "\n",
    "for x, _ in get_friedman():\n",
    "    metric.update(x, model.predict_one(x), model.centers)\n",
    "    model.learn_one(x)\n",
    "\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "\n",
    "We can go much deeper into online machine learning solutions. There are multiple strategies to combine models, selecting the best model among a set thereof, and many other aspects. Online hyperparameter tuning is also an exciting research area.\n",
    "\n",
    "I strongly suggest checking these additional resources to learn more about online machine learning:\n",
    "\n",
    "**Tutorials:**\n",
    "\n",
    "- [The art of using pipelines](https://riverml.xyz/latest/examples/the-art-of-using-pipelines/)\n",
    "- [Working with imbalanced data](https://riverml.xyz/dev/examples/imbalanced-learning/)\n",
    "- [Debbuging a pipeline](https://riverml.xyz/dev/examples/debugging-a-pipeline/)\n",
    "\n",
    "**Resource hub:**\n",
    "\n",
    "- [Awesome online machine learning](https://github.com/online-ml/awesome-online-machine-learning)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Thank you so much for having me!\n",
    "\n",
    "Do you have any questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('river')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b27b2a9f272874e098660428b28f5afa65c7850d82ff592660d49a141d883cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
